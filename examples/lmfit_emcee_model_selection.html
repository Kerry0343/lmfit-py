<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Selection using lmfit and emcee &#8212; Non-Linear Least-Squares Minimization and Curve-Fitting for Python</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx13.css?v=aac77d10" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css?v=572af1d6" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=bb516dca"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Calculate Confidence Intervals" href="example_confidence_interval.html" />
    <link rel="prev" title="Complex Resonator Model" href="example_complex_resonator_model.html" />
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 
    <style type="text/css">
      table.right { float: right; margin-left: 20px; }
      table.right td { border: 1px solid #ccc; }
    </style>
    <script>
      // intelligent scrolling of the sidebar content
      $(window).scroll(function() {
        var sb = $('.sphinxsidebarwrapper');
        var win = $(window);
        var sbh = sb.height();
        var offset = $('.sphinxsidebar').position()['top'];
        var wintop = win.scrollTop();
        var winbot = wintop + win.innerHeight();
        var curtop = sb.position()['top'];
        var curbot = curtop + sbh;
        // does sidebar fit in window?
        if (sbh < win.innerHeight()) {
          // yes: easy case -- always keep at the top
          sb.css('top', $u.min([$u.max([0, wintop - offset - 10]),
                                $(document).height() - sbh - 200]));
        } else {
          // no: only scroll if top/bottom edge of sidebar is at
          // top/bottom edge of window
          if (curtop > wintop && curbot > winbot) {
            sb.css('top', $u.max([wintop - offset - 10, 0]));
          } else if (curtop < wintop && curbot < winbot) {
            sb.css('top', $u.min([winbot - sbh - offset - 20,
                                  $(document).height() - sbh - 200]));
          }
        }
      });
    </script>

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="../contents.html">Contents</a></li>
    <li><a href="index.html">Examples</a></li>
    <li><a href="../installation.html">Installation</a></li>
    <li><a href="../faq.html">FAQ</a></li>
    <li><a href="../support.html">Support</a></li>
    <li><a href="https://github.com/lmfit/lmfit-py">Develop</a></li>
  </ul>
  <div>
    <a href="../index.html">
      <img src="../_static/lmfitheader.png" alt="LMFIT" />
    </a>
  </div>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="example_confidence_interval.html" title="Calculate Confidence Intervals"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="example_complex_resonator_model.html" title="Complex Resonator Model"
             accesskey="P">previous</a> |</li>
    <li>[ <a href="../intro.html">intro</a> |</li>
    <li><a href="../parameters.html">parameters</a> |</li>
    <li><a href="../fitting.html">minimize</a> |</li>
    <li><a href="../model.html">model</a> |</li>
    <li><a href="../builtin_models.html">built-in models</a> |</li>
    <li><a href="../confidence.html">confidence intervals</a> |</li>
    <li><a href="../bounds.html">bounds</a> |</li>
    <li><a href="../constraints.html">constraints</a> ]</li>

      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="example_complex_resonator_model.html"
                          title="previous chapter">Complex Resonator Model</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="example_confidence_interval.html"
                          title="next chapter">Calculate Confidence Intervals</a></p>
  </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-examples-lmfit-emcee-model-selection-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="model-selection-using-lmfit-and-emcee">
<span id="sphx-glr-examples-lmfit-emcee-model-selection-py"></span><h1>Model Selection using lmfit and emcee<a class="headerlink" href="#model-selection-using-lmfit-and-emcee" title="Link to this heading">¶</a></h1>
<p>FIXME: this is a useful example; however, it doesn’t run correctly anymore as
the PTSampler was removed in emcee v3…</p>
<p><cite>lmfit.emcee</cite> can be used to obtain the posterior probability distribution
of parameters, given a set of experimental data. This notebook shows how it
can be used for Bayesian model selection.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">lmfit</span>
</pre></div>
</div>
<p>Define a Gaussian lineshape and generate some data:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a_max</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">sd</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a_max</span> <span class="o">*</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html#numpy.exp" title="numpy.exp" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">exp</span></a><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">loc</span><span class="p">)</span> <span class="o">/</span> <span class="n">sd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>


<span class="n">x</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
<a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html#numpy.random.seed" title="numpy.random.seed" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="n">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mf">5.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">dy</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html#numpy.sqrt" title="numpy.sqrt" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span></a><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span> <span class="o">+=</span> <span class="n">dy</span> <span class="o">*</span> <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html#numpy.random.randn" title="numpy.random.randn" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
<p>Plot the data:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html#matplotlib.pyplot.errorbar" title="matplotlib.pyplot.errorbar" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span></a><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Define the normalised residual for the data:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">residual</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">just_generative</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">valuesdict</span><span class="p">()</span>
    <span class="n">generative</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">v</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">M</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="sa">f</span><span class="s1">&#39;a_max</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">&#39;</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span>
        <span class="n">generative</span> <span class="o">+=</span> <span class="n">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;a_max</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;loc</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;sd</span><span class="si">{</span><span class="n">M</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">])</span>
        <span class="n">M</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">just_generative</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">generative</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">generative</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">dy</span>
</pre></div>
</div>
<p>Create a Parameter set for the initial guesses:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initial_peak_params</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">lmfit</span><span class="o">.</span><span class="n">Parameters</span><span class="p">()</span>

    <span class="c1"># a and b give a linear background</span>
    <span class="n">a</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html#numpy.mean" title="numpy.mean" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">mean</span></a><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># a_max, loc and sd are the amplitude, location and SD of each Gaussian</span>
    <span class="c1"># component</span>
    <span class="n">a_max</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">loc</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.mean.html#numpy.mean" title="numpy.mean" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">mean</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="p">(</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.min.html#numpy.min" title="numpy.min" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">min</span></a><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>

    <span class="n">p</span><span class="o">.</span><span class="n">add_many</span><span class="p">((</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
        <span class="n">p</span><span class="o">.</span><span class="n">add_many</span><span class="p">((</span><span class="sa">f</span><span class="s1">&#39;a_max</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">a_max</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">a_max</span><span class="p">),</span>
                   <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loc</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.min.html#numpy.min" title="numpy.min" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">min</span></a><span class="p">(</span><span class="n">x</span><span class="p">),</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">x</span><span class="p">)),</span>
                   <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sd</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.min.html#numpy.min" title="numpy.min" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">min</span></a><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
<p>Solving with <cite>minimize</cite> gives the Maximum Likelihood solution.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">p1</span> <span class="o">=</span> <span class="n">initial_peak_params</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mi1</span> <span class="o">=</span> <span class="n">lmfit</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;differential_evolution&#39;</span><span class="p">)</span>

<span class="n">lmfit</span><span class="o">.</span><span class="n">printfuncs</span><span class="o">.</span><span class="n">report_fit</span><span class="p">(</span><span class="n">mi1</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">min_correl</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>From inspection of the data above we can tell that there is going to be more
than 1 Gaussian component, but how many are there? A Bayesian approach can
be used for this model selection problem. We can do this with <cite>lmfit.emcee</cite>,
which uses the <cite>emcee</cite> package to do a Markov Chain Monte Carlo sampling of
the posterior probability distribution. <cite>lmfit.emcee</cite> requires a function
that returns the log-posterior probability. The log-posterior probability is
a sum of the log-prior probability and log-likelihood functions.</p>
<p>The log-prior probability encodes information about what you already believe
about the system. <cite>lmfit.emcee</cite> assumes that this log-prior probability is
zero if all the parameters are within their bounds and <cite>-np.inf</cite> if any of
the parameters are outside their bounds. As such it’s a uniform prior.</p>
<p>The log-likelihood function is given below. To use non-uniform priors then
should include these terms in <cite>lnprob</cite>. This is the log-likelihood
probability for the sampling.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lnprob</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">resid</span> <span class="o">=</span> <span class="n">residual</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">just_generative</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html#numpy.sum" title="numpy.sum" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">sum</span></a><span class="p">(((</span><span class="n">resid</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">dy</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log" title="numpy.log" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">log</span></a><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <a href="https://numpy.org/doc/stable/reference/constants.html#numpy.pi" title="numpy.pi" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">pi</span></a> <span class="o">*</span> <span class="n">dy</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>To start with we have to create the minimizers and <em>burn</em> them in. We create
4 different minimizers representing 0, 1, 2 or 3 Gaussian contributions. To
do the model selection we have to integrate the over the log-posterior
distribution to see which has the higher probability. This is done using the
<cite>thermodynamic_integration_log_evidence</cite> method of the <cite>sampler</cite> attribute
contained in the <cite>lmfit.Minimizer</cite> object.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Work out the log-evidence for different numbers of peaks:</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="mi">310</span>
<span class="n">burn</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">thin</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">ntemps</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">workers</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># the multiprocessing does not work with sphinx-gallery</span>
<span class="n">log_evidence</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># set up the Minimizers</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">p0</span> <span class="o">=</span> <span class="n">initial_peak_params</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="c1"># you can&#39;t use lnprob as a userfcn with minimize because it needs to be</span>
    <span class="c1"># maximised</span>
    <span class="n">mini</span> <span class="o">=</span> <span class="n">lmfit</span><span class="o">.</span><span class="n">Minimizer</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span> <span class="n">p0</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">mini</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;differential_evolution&#39;</span><span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">mini</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># burn in the samplers</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="c1"># do the sampling</span>
    <span class="n">mini</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lmfit</span><span class="o">.</span><span class="n">Minimizer</span><span class="p">(</span><span class="n">lnprob</span><span class="p">,</span> <span class="n">res</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">emcee</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">,</span> <span class="n">ntemps</span><span class="o">=</span><span class="n">ntemps</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span>
                        <span class="n">reuse_sampler</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">float_behavior</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span>
                        <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># get the evidence</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">thermodynamic_integration_log_evidence</span><span class="p">())</span>
    <span class="n">log_evidence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">thermodynamic_integration_log_evidence</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>Once we’ve burned in the samplers we have to do a collection run. We thin
out the MCMC chain to reduce autocorrelation between successive samples.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">total_steps</span> <span class="o">+=</span> <span class="mi">100</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="c1"># do the sampling</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">emcee</span><span class="p">(</span><span class="n">burn</span><span class="o">=</span><span class="n">burn</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="n">thin</span><span class="p">,</span> <span class="n">ntemps</span><span class="o">=</span><span class="n">ntemps</span><span class="p">,</span>
                            <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">reuse_sampler</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># get the evidence</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">thermodynamic_integration_log_evidence</span><span class="p">())</span>
        <span class="n">log_evidence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">thermodynamic_integration_log_evidence</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>


<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" title="matplotlib.pyplot.plot" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span></a><span class="p">(</span><span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:])</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylabel.html#matplotlib.pyplot.ylabel" title="matplotlib.pyplot.ylabel" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s1">&#39;Log-evidence&#39;</span><span class="p">)</span>
<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html#matplotlib.pyplot.xlabel" title="matplotlib.pyplot.xlabel" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s1">&#39;number of peaks&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The Bayes factor is related to the exponential of the difference between the
log-evidence values.  Thus, 0 peaks is not very likely compared to 1 peak.
But 1 peak is not as good as 2 peaks. 3 peaks is not that much better than 2
peaks.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">r01</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html#numpy.exp" title="numpy.exp" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">exp</span></a><span class="p">(</span><span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">r12</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html#numpy.exp" title="numpy.exp" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">exp</span></a><span class="p">(</span><span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="n">r23</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html#numpy.exp" title="numpy.exp" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-data"><span class="n">np</span><span class="o">.</span><span class="n">exp</span></a><span class="p">(</span><span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">log_evidence</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">r01</span><span class="p">,</span> <span class="n">r12</span><span class="p">,</span> <span class="n">r23</span><span class="p">)</span>
</pre></div>
</div>
<p>These numbers tell us that zero peaks is 0 times as likely as one peak. Two
peaks is 7e49 times more likely than one peak. Three peaks is 1.1 times more
likely than two peaks. With this data one would say that two peaks is
sufficient. Caution has to be taken with these values. The log-priors for
this sampling are uniform but improper, i.e. they are not normalised properly.
Internally the lnprior probability is calculated as 0 if all parameters are
within their bounds and <cite>-np.inf</cite> if any parameter is outside the bounds.
The <cite>lnprob</cite> function defined above is the log-likelihood alone. Remember,
that the log-posterior probability is equal to the sum of the log-prior and
log-likelihood probabilities. Extra terms can be added to the lnprob function
to calculate the normalised log-probability. These terms would look something
like:</p>
<div class="math notranslate nohighlight">
\[\log (\prod_i \frac{1}{max_i - min_i})\]</div>
<p>where <span class="math notranslate nohighlight">\(max_i\)</span> and <span class="math notranslate nohighlight">\(min_i\)</span> are the upper and lower bounds for the
parameter, and the prior is a uniform distribution. Other types of prior are
possible. For example, you might expect the prior to be Gaussian.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-lmfit-emcee-model-selection-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/9074bca346e91d4783f1ebfda08595dd/lmfit_emcee_model_selection.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">lmfit_emcee_model_selection.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c5b1d1ce60f76e20b3312a2697eca7f1/lmfit_emcee_model_selection.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">lmfit_emcee_model_selection.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="example_confidence_interval.html" title="Calculate Confidence Intervals"
             >next</a> |</li>
        <li class="right" >
          <a href="example_complex_resonator_model.html" title="Complex Resonator Model"
             >previous</a> |</li>
    <li>[ <a href="../intro.html">intro</a> |</li>
    <li><a href="../parameters.html">parameters</a> |</li>
    <li><a href="../fitting.html">minimize</a> |</li>
    <li><a href="../model.html">model</a> |</li>
    <li><a href="../builtin_models.html">built-in models</a> |</li>
    <li><a href="../confidence.html">confidence intervals</a> |</li>
    <li><a href="../bounds.html">bounds</a> |</li>
    <li><a href="../constraints.html">constraints</a> ]</li>

          <li class="nav-item nav-item-1"><a href="index.html" >Examples gallery</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2024, Matthew Newville, Till Stensitzki, Renee Otten, and others.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>