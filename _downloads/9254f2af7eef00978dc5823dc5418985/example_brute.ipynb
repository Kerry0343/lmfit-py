{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Global minimization using the ``brute`` method (a.k.a. grid search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows a simple example of using ``lmfit.minimize.brute`` that\nuses the method with the same name from ``scipy.optimize``.\n\nThe method computes the function\u2019s value at each point of a multidimensional\ngrid of points, to find the global minimum of the function. It behaves\nidentically to ``scipy.optimize.brute`` in case finite bounds are given on\nall varying parameters, but will also deal with non-bounded parameters\n(see below).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\n\nfrom matplotlib.colors import LogNorm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom lmfit import Minimizer, create_params, fit_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with the example given in the documentation of SciPy:\n\n\"We illustrate the use of brute to seek the global minimum of a function of\ntwo variables that is given as the sum of a positive-definite quadratic and\ntwo deep \u201cGaussian-shaped\u201d craters. Specifically, define the objective\nfunction ``f`` as the sum of three other functions, ``f = f1 + f2 + f3``. We\nsuppose each of these has a signature ``(z, *params), where z = (x, y)``,\nand params and the functions are as defined below.\"\n\nFirst, we create a set of Parameters where all variables except ``x`` and\n``y`` are given fixed values.\nJust as in the documentation we will do a grid search between ``-4`` and\n``4`` and use a stepsize of ``0.25``. The bounds can be set as usual with\nthe ``min`` and ``max`` attributes, and the stepsize is set using\n``brute_step``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = create_params(a=dict(value=2, vary=False),\n                       b=dict(value=3, vary=False),\n                       c=dict(value=7, vary=False),\n                       d=dict(value=8, vary=False),\n                       e=dict(value=9, vary=False),\n                       f=dict(value=10, vary=False),\n                       g=dict(value=44, vary=False),\n                       h=dict(value=-1, vary=False),\n                       i=dict(value=2, vary=False),\n                       j=dict(value=26, vary=False),\n                       k=dict(value=1, vary=False),\n                       l=dict(value=-2, vary=False),\n                       scale=dict(value=0.5, vary=False),\n                       x=dict(value=0.0, vary=True, min=-4, max=4, brute_step=0.25),\n                       y=dict(value=0.0, vary=True, min=-4, max=4, brute_step=0.25))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, create the three functions and the objective function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def f1(p):\n    par = p.valuesdict()\n    return (par['a'] * par['x']**2 + par['b'] * par['x'] * par['y'] +\n            par['c'] * par['y']**2 + par['d']*par['x'] + par['e']*par['y'] +\n            par['f'])\n\n\ndef f2(p):\n    par = p.valuesdict()\n    return (-1.0*par['g']*np.exp(-((par['x']-par['h'])**2 +\n                                   (par['y']-par['i'])**2) / par['scale']))\n\n\ndef f3(p):\n    par = p.valuesdict()\n    return (-1.0*par['j']*np.exp(-((par['x']-par['k'])**2 +\n                                   (par['y']-par['l'])**2) / par['scale']))\n\n\ndef f(params):\n    return f1(params) + f2(params) + f3(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the actual grid search is done with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitter = Minimizer(f, params)\nresult = fitter.minimize(method='brute')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ", which will increment ``x`` and ``y`` between ``-4`` in increments of\n``0.25`` until ``4`` (not inclusive).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid_x, grid_y = (np.unique(par.ravel()) for par in result.brute_grid)\nprint(grid_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The objective function is evaluated on this grid, and the raw output from\n``scipy.optimize.brute`` is stored in the MinimizerResult as\n``brute_<parname>`` attributes. These attributes are:\n\n``result.brute_x0`` -- A 1-D array containing the coordinates of a point at\nwhich the objective function had its minimum value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_fval`` -- Function value at the point ``x0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_fval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_grid`` -- Representation of the evaluation grid. It has the\nsame length as ``x0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``result.brute_Jout`` -- Function values at each point of the evaluation\ngrid, i.e., ``Jout = func(*grid)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result.brute_Jout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reassuringly, the obtained results are identical to using the method in\nSciPy directly!**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example 2: fit of a decaying sine wave\n\nIn this example, we will explain some of the options of the algorithm.\n\nWe start off by generating some synthetic data with noise for a decaying sine\nwave, define an objective function, and create/initialize a Parameter set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0, 15, 301)\nnp.random.seed(7)\nnoise = np.random.normal(size=x.size, scale=0.2)\ndata = (5. * np.sin(2*x - 0.1) * np.exp(-x*x*0.025) + noise)\nplt.plot(x, data, 'o')\nplt.show()\n\n\ndef fcn2min(params, x, data):\n    \"\"\"Model decaying sine wave, subtract data.\"\"\"\n    amp = params['amp']\n    shift = params['shift']\n    omega = params['omega']\n    decay = params['decay']\n    model = amp * np.sin(x*omega + shift) * np.exp(-x*x*decay)\n    return model - data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In contrast to the implementation in SciPy (as shown in the first example),\nvarying parameters do not need to have finite bounds in lmfit. However, if a\nparameter does not have finite bounds, then it does need a ``brute_step``\nattribute specified:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = create_params(amp=dict(value=7, min=2.5, brute_step=0.25),\n                       decay=dict(value=0.05, brute_step=0.005),\n                       shift=dict(value=0.0, min=-np.pi/2., max=np.pi/2),\n                       omega=dict(value=3, max=5, brute_step=0.25))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our initial parameter set is now defined as shown below and this will\ndetermine how the grid is set-up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we initialize a ``Minimizer`` and perform the grid search:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitter = Minimizer(fcn2min, params, fcn_args=(x, data))\nresult_brute = fitter.minimize(method='brute', Ns=25, keep=25)\n\nprint(fit_report(result_brute))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used two new parameters here: ``Ns`` and ``keep``. The parameter ``Ns``\ndetermines the \\'number of grid points along the axes\\' similarly to its usage\nin SciPy. Together with ``brute_step``, ``min`` and ``max`` for a Parameter\nit will dictate how the grid is set-up:\n\n**(1)** finite bounds are specified (\"SciPy implementation\"): uses\n``brute_step`` if present (in the example above) or uses ``Ns`` to generate\nthe grid. The latter scenario that interpolates ``Ns`` points from ``min``\nto ``max`` (inclusive), is here shown for the parameter ``shift``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'shift'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(f\"parameter = {par_name}\\nnumber of steps = {len(grid_shift)}\\ngrid = {grid_shift}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If finite bounds are not set for a certain parameter then the user **must**\nspecify ``brute_step`` - three more scenarios are considered here:\n\n**(2)** lower bound ``(min``) and ``brute_step`` are specified:\n``range = (min, min + Ns * brute_step, brute_step)``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'amp'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(f\"parameter = {par_name}\\nnumber of steps = {len(grid_shift)}\\ngrid = {grid_shift}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(3)** upper bound (``max``) and ``brute_step`` are specified:\n``range = (max - Ns * brute_step, max, brute_step)``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'omega'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(f\"parameter = {par_name}\\nnumber of steps = {len(grid_shift)}\\ngrid = {grid_shift}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(4)** numerical value (``value``) and ``brute_step`` are specified:\n``range = (value - (Ns//2) * brute_step, value + (Ns//2) * brute_step, brute_step)``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "par_name = 'decay'\nindx_shift = result_brute.var_names.index(par_name)\ngrid_shift = np.unique(result_brute.brute_grid[indx_shift].ravel())\nprint(f\"parameter = {par_name}\\nnumber of steps = {len(grid_shift)}\\ngrid = {grid_shift}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``MinimizerResult`` contains all the usual best-fit parameters and\nfitting statistics. For example, the optimal solution from the grid search\nis given below together with a plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fit_report(result_brute))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(x, data, 'o')\nplt.plot(x, data + fcn2min(result_brute.params, x, data), '--')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this fit is already very good, which is what we should expect\nsince our ``brute`` force grid is sampled rather finely and encompasses the\n\"correct\" values.\n\nIn a more realistic, complicated example the ``brute`` method will be used\nto get reasonable values for the parameters and perform another minimization\n(e.g., using ``leastsq``) using those as starting values. That is where the\n``keep`` parameter comes into play: it determines the \"number of best\ncandidates from the brute force method that are stored in the ``candidates``\nattribute\". In the example above we store the best-ranking 25 solutions (the\ndefault value is ``50`` and storing all the grid points can be accomplished\nby choosing ``all``). The ``candidates`` attribute contains the parameters\nand ``chisqr`` from the brute force method as a ``namedtuple``,\n``(\u2018Candidate\u2019, [\u2018params\u2019, \u2018score\u2019])``, sorted on the (lowest) ``chisqr``\nvalue. To access the values for a particular candidate one can use\n``result.candidate[#].params`` or ``result.candidate[#].score``, where a\nlower # represents a better candidate. The ``show_candidates(#)`` uses the\n``pretty_print()`` method to show a specific candidate-# or all candidates\nwhen no number is specified.\n\nThe optimal fit is, as usual, stored in the ``MinimizerResult.params``\nattribute and is, therefore, identical to ``result_brute.show_candidates(1)``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_brute.show_candidates(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the next-best scoring candidate has already a ``chisqr`` that\nincreased quite a bit:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_brute.show_candidates(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and is, therefore, probably not so likely... However, as said above, in most\ncases you'll want to do another minimization using the solutions from the\n``brute`` method as starting values. That can be easily accomplished as\nshown in the code below, where we now perform a ``leastsq`` minimization\nstarting from the top-25 solutions and accept the solution if the ``chisqr``\nis lower than the previously 'optimal' solution:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_result = copy.deepcopy(result_brute)\n\nfor candidate in result_brute.candidates:\n    trial = fitter.minimize(method='leastsq', params=candidate.params)\n    if trial.chisqr < best_result.chisqr:\n        best_result = trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the ``leastsq`` minimization we obtain the following parameters for the\nmost optimal result:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fit_report(best_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected the parameters have not changed significantly as they were\nalready very close to the \"real\" values, which can also be appreciated from\nthe plots below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(x, data, 'o')\nplt.plot(x, data + fcn2min(result_brute.params, x, data), '-',\n         label='brute')\nplt.plot(x, data + fcn2min(best_result.params, x, data), '--',\n         label='brute followed by leastsq')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the results from the ``brute`` force grid-search can be visualized\nusing the rather lengthy Python function below (which might get incorporated\nin lmfit at some point).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_results_brute(result, best_vals=True, varlabels=None,\n                       output=None):\n    \"\"\"Visualize the result of the brute force grid search.\n\n    The output file will display the chi-square value per parameter and contour\n    plots for all combination of two parameters.\n\n    Inspired by the `corner` package (https://github.com/dfm/corner.py).\n\n    Parameters\n    ----------\n    result : :class:`~lmfit.minimizer.MinimizerResult`\n        Contains the results from the :meth:`brute` method.\n\n    best_vals : bool, optional\n        Whether to show the best values from the grid search (default is True).\n\n    varlabels : list, optional\n        If None (default), use `result.var_names` as axis labels, otherwise\n        use the names specified in `varlabels`.\n\n    output : str, optional\n        Name of the output PDF file (default is 'None')\n    \"\"\"\n    npars = len(result.var_names)\n    _fig, axes = plt.subplots(npars, npars)\n\n    if not varlabels:\n        varlabels = result.var_names\n    if best_vals and isinstance(best_vals, bool):\n        best_vals = result.params\n\n    for i, par1 in enumerate(result.var_names):\n        for j, par2 in enumerate(result.var_names):\n\n            # parameter vs chi2 in case of only one parameter\n            if npars == 1:\n                axes.plot(result.brute_grid, result.brute_Jout, 'o', ms=3)\n                axes.set_ylabel(r'$\\chi^{2}$')\n                axes.set_xlabel(varlabels[i])\n                if best_vals:\n                    axes.axvline(best_vals[par1].value, ls='dashed', color='r')\n\n            # parameter vs chi2 profile on top\n            elif i == j and j < npars-1:\n                if i == 0:\n                    axes[0, 0].axis('off')\n                ax = axes[i, j+1]\n                red_axis = tuple(a for a in range(npars) if a != i)\n                ax.plot(np.unique(result.brute_grid[i]),\n                        np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                        'o', ms=3)\n                ax.set_ylabel(r'$\\chi^{2}$')\n                ax.yaxis.set_label_position(\"right\")\n                ax.yaxis.set_ticks_position('right')\n                ax.set_xticks([])\n                if best_vals:\n                    ax.axvline(best_vals[par1].value, ls='dashed', color='r')\n\n            # parameter vs chi2 profile on the left\n            elif j == 0 and i > 0:\n                ax = axes[i, j]\n                red_axis = tuple(a for a in range(npars) if a != i)\n                ax.plot(np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                        np.unique(result.brute_grid[i]), 'o', ms=3)\n                ax.invert_xaxis()\n                ax.set_ylabel(varlabels[i])\n                if i != npars-1:\n                    ax.set_xticks([])\n                else:\n                    ax.set_xlabel(r'$\\chi^{2}$')\n                if best_vals:\n                    ax.axhline(best_vals[par1].value, ls='dashed', color='r')\n\n            # contour plots for all combinations of two parameters\n            elif j > i:\n                ax = axes[j, i+1]\n                red_axis = tuple(a for a in range(npars) if a not in (i, j))\n                X, Y = np.meshgrid(np.unique(result.brute_grid[i]),\n                                   np.unique(result.brute_grid[j]))\n                lvls1 = np.linspace(result.brute_Jout.min(),\n                                    np.median(result.brute_Jout)/2.0, 7, dtype='int')\n                lvls2 = np.linspace(np.median(result.brute_Jout)/2.0,\n                                    np.median(result.brute_Jout), 3, dtype='int')\n                lvls = np.unique(np.concatenate((lvls1, lvls2)))\n                ax.contourf(X.T, Y.T, np.minimum.reduce(result.brute_Jout, axis=red_axis),\n                            lvls, norm=LogNorm())\n                ax.set_yticks([])\n                if best_vals:\n                    ax.axvline(best_vals[par1].value, ls='dashed', color='r')\n                    ax.axhline(best_vals[par2].value, ls='dashed', color='r')\n                    ax.plot(best_vals[par1].value, best_vals[par2].value, 'rs', ms=3)\n                if j != npars-1:\n                    ax.set_xticks([])\n                else:\n                    ax.set_xlabel(varlabels[i])\n                if j - i >= 2:\n                    axes[i, j].axis('off')\n\n    if output is not None:\n        plt.savefig(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and finally, to generated the figure:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_results_brute(result_brute, best_vals=True, varlabels=None)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}